{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" , style=\"line-height: 1.9\">\n",
    "<h1 style=\"color:lightBlue;\">نرمالسازی متن</h1>\n",
    "<p style=\"font-size: 17px;\">نرمال‌سازی متن فرآیند تبدیل متن به یک قالب ثابت و یکنواخت است که درک و پردازش متن را برای کامپیوتر آسان‌تر می‌کند</p>\n",
    "<h4 style=\"color:lightgreen;\"><li> اهمیت نرمالسازی متن:</h4>\n",
    "<ul style=\"font-size: 17px;\">\n",
    "<li>بهبود کارایی: با تبدیل متن به یک قالب استاندارد، کامپیوتر می‌توانند اطلاعات را کارآمدتر مدیریت کنند. تصور کنید به دنبال \"donut\" می‌گردید - نرمالسازی اطمینان می‌دهد که نتایج شامل  (doughnut) نیز می‌شود\n",
    "</li>\n",
    "\n",
    "<li>دقت بیشتر: نرمالسازی ابهام متن را کاهش می‌دهد. برای مثال، \"۲۰۰\" می‌تواند نشان‌دهنده‌ی ارز یا یک عدد باشد. نرمال‌سازی این موضوع را برای کارهایی مانند تبدیل متن به گفتار یا تحلیل داده‌ها روشن می‌کند\n",
    "</li>\n",
    "<li>تحلیل بهتر: نرمالسازی به آماده‌کردن متن برای وظایف پردازش زبان طبیعی (NLP) کمک می‌کند. برنامه‌های کاربردی NLP مانند تحلیل احساسات یا ترجمه ماشینی از متن نرمال شده بهره می‌برند\n",
    "</li>\n",
    "</ul>\n",
    "<h4 style=\"color:lightgreen;\"> <li>برای عادی‌سازی متن، بسته به هدف، از تکنیک‌های مختلفی استفاده می‌شود:</h4>\n",
    "<ul style=\"font-size: 17px;\">\n",
    "<li>تغییر حروف: تبدیل تمام متن به حروف کوچک یا بزرگ و استفاده از یونیکد ,حذف کلمات تلفظ </li>\n",
    "<li>بسط اختصارات و حروف اختصاری: بسط اختصارات مانند \"سازمان ملل متحد\" به جای \"UN\"</li>\n",
    "<li>حذف علائم نگارش: حذف علائم نگارش مانند کاما و نقطه</li>\n",
    "<li>ریشه‌‌یابی و واژه‌یابی: این تکنیک‌ها کلمات را به ریشه‌ی آن‌ها کاهش می‌دهند (برای مثال، \"running\" به \"run\" تبدیل می‌شود). ریشه‌یابی ساده‌تر است، اما همیشه ممکن است کلمات واقعی تولید نکند، در حالی که واژه‌یابی با در نظر گرفتن زمینه، شکل پایه صحیح را انتخاب می‌کند (برای مثال، \"بازی می‌کند\" برای فعل به \"بازی\" و برای اسم به \"بازیکن\" تبدیل می‌شود</li>\n",
    "</ul>\n",
    "<h4 style=\"color:lightgreen;\"><li> نرمالسازی متن در بسیاری از برنامه‌های کاربردی  ضروری است:</li></h4>\n",
    "<ul style=\"font-size: 17px;\">\n",
    "<li>بازیابی اطلاعات: موتورهای جستجو از نرمالسازی برای بهبود دقت جستجو با در نظر گرفتن عبارات مختلف یک مفهوم استفاده می‌کنند.\n",
    "</li>\n",
    "<li>ترجمه ماشینی: عادی‌سازی به ابزارهای ترجمه کمک می‌کند تا معنای واقعی متن را درک کنند و در نتیجه به ترجمه‌های دقیق‌تر منجر شود.\n",
    "</li>\n",
    "<li>تحلیل متن: تکنیک‌هایی مانند تحلیل احساسات به متن ندمالشده متکی هستند تا به درستی احساسات منتقل شده در نوشتار را ارزیابی کنند.\n",
    "</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read parquet database as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to Parquet file\n",
    "dataset = pd.read_parquet(\"../data/imdb/plain_text/train-00000-of-00001.parquet\")\n",
    "# Access text data (column name is 'text')\n",
    "text_data = dataset[\"text\"]\n",
    "# Access class label (column name is 'label')\n",
    "class_labels = dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(text):\n",
    "    \"\"\"\n",
    "    preprocess the IMdb dataset\n",
    "\n",
    "    Args:\n",
    "        text (str): The text of the movie review.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text file\n",
    "    \"\"\"\n",
    "    # Convert to Unicode and remove Accents(ç, Ç ,â, î, ô, ü) and Lowercase conversion\n",
    "    text = unidecode.unidecode(text).lower()\n",
    "\n",
    "    # Number replacement( 2 --> two) , \"re.sub\" Replaces one or many matches with a string\n",
    "    # m.group(0) capturing group  from a regular expression match stored in variable \n",
    "    text = re.sub(r\"\\d+\", lambda m: num2words.num2words(int(m.group(0))), text)\n",
    "    \n",
    "    # Remove punctuation(; ,{}, [ < ] ) , non-alphanumeric(&, $, @, %, - ...) characters , URLs , HTML tags\n",
    "    pattern = r\"\\n|[^\\w\\s]|(https?://\\S+)|<.*?>\"\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words (a, an, and, are, as, at, be, but, by, for, if ,...)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # lemmatization( running --> run )\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save strings to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "preprocessed_text = []\n",
    "for review in text_data:\n",
    "    preprocessed_text.append(preprocess_review(review))\n",
    "\n",
    "# Convert preprocessed data to pandas data frame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"original_text\": text_data,\n",
    "        \"preprocessed_text\": preprocessed_text,\n",
    "        \"label\": class_labels,\n",
    "    }\n",
    ")\n",
    "# Save as CSV\n",
    "df.to_csv(\"preprocessed_data_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>rent curiousyellow video store controversy sur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>curious yellow risible pretentious steam pile ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>avoid make type film future film interest expe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>film probably inspire godards masculin feminin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>oh brotherafter hear ridiculous film umpteen y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...   \n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...   \n",
       "2  If only to avoid making this type of film in t...   \n",
       "3  This film was probably inspired by Godard's Ma...   \n",
       "4  Oh, brother...after hearing about this ridicul...   \n",
       "\n",
       "                                   preprocessed_text  label  \n",
       "0  rent curiousyellow video store controversy sur...      0  \n",
       "1  curious yellow risible pretentious steam pile ...      0  \n",
       "2  avoid make type film future film interest expe...      0  \n",
       "3  film probably inspire godards masculin feminin...      0  \n",
       "4  oh brotherafter hear ridiculous film umpteen y...      0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of word occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'good' appears 15640 times\n"
     ]
    }
   ],
   "source": [
    "def count_word(df, word):\n",
    "    \"\"\"\n",
    "    count word occurrence in each row of column\n",
    "\n",
    "    Args:\n",
    "        df (any): dataframe \n",
    "        word (str): string lookup\n",
    "\n",
    "    Returns:\n",
    "        int: summation of counted word in each row \n",
    "    \"\"\"\n",
    "    return df[\"preprocessed_text\"].str.count(word).sum()\n",
    "\n",
    "\n",
    "# Count occurrences\n",
    "find_word = input()\n",
    "count_time = count_word(df.copy(), find_word)\n",
    "\n",
    "\n",
    "print(f\"The word '{find_word}' appears {count_time} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"The word '{find_word}' appears {(lambda df,word : df[\"preprocessed_text\"].str.count(word).sum())(df.copy(), input())} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
